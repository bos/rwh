<!-- vim: set filetype=docbkxml shiftwidth=2 autoindent expandtab tw=77 : -->

<chapter id="concurrent">
  <title>Concurrent and multicore programming</title>

  <para id="x_X31">As we write this book, the landscape of CPU architecture is
    changing more rapidly than it has in decades.  </para>

  <sect1>
    <title>Defining concurrency and parallelism</title>

    <para id="x_Y31">A <emphasis>concurrent</emphasis> program needs
      to perform several possibly unrelated tasks at the same time.
      Consider the example of a game server: it is typically composed
      of dozens of components, each of which has complicated
      interactions with the outside world.  One component might handle
      multi-user chat; several more will process the inputs of
      players, and feed state updates back to them; while another
      performs physics calculations.</para>

    <para id="x_Z31">The correct operation of a concurrent program does not
      require multiple cores, though they may improve performance and
      responsiveness.</para>

    <para id="x_a31">In contrast, a <emphasis>parallel</emphasis>
      program solves a single problem.  Consider a financial model
      that attempts to predict the next minute of fluctuations in the
      price of a single stock.  If we want to apply this model to
      every stock listed on an exchange, for example to estimate which
      ones we should buy and sell, we hope to get an answer more
      quickly if we run the model on five hundred cores than if we use
      just one. As this suggests, a parallel program does not usually
      depend on the presence of multiple cores to work
      correctly.</para>

    <para id="x_b31">Another useful distinction between concurrent and parallel
      programs lies in their interaction with the outside world.  By
      definition, a concurrent program deals continuously with
      networking protocols, databases, and the like.  A typical
      parallel program is likely to be more focused: it streams data
      in, crunches it for a while (with little further I/O), then
      streams data back out.</para>

    <note>
      <para>Many traditional languages further blur the already
	indistinct boundary between concurrent and parallel
	programming, because they force programmers to use the same
	primitives to construct both kinds of program.</para>
    </note>

    <para id="x_c31">In this chapter, we will concern ourselves with concurrent
      and parallel programs that operate within the boundaries of a
      single operating system process.</para>
  </sect1>

  <sect1>
    <title>Concurrent programming with threads</title>

    <para id="x_d31">As a building block for concurrent programs, most
      programming languages provide a way of creating multiple
      independent <emphasis>threads of control</emphasis>.  Haskell is
      no exception, though programming with threads in Haskell looks
      somewhat different than in other languages.</para>

    <para id="x_e31">In Haskell, a thread is an <type>IO</type> action that
      executes independently from other threads.  To create a thread,
      we import the <code>Control.Concurrent</code> module and use the
      <function>forkIO</function> function.</para>

    &forkIO.ghci:forkIO;

    <para id="x_f31">The new thread starts to execute almost immediately, and the
      thread that created it continues to execute concurrently.  The
      thread will stop executing when it reaches the end of its &IO;
      action.</para>

    <sect2>
      <title>Threads are nondeterministic</title>

      <para id="x_g31">The runtime component of &GHC; does not specify an order
	in which it executes threads.  As a result, in our example
	above, the file <filename>xyzzy</filename> created by the new
	thread <emphasis>may or may not</emphasis> have been created
	by the time the original thread checks for its existence. If
	we try this example once, then remove
	<filename>xyzzy</filename> and try again, we may get a
	different result the second time.</para>
    </sect2>

    <sect2>
      <title>Hiding latency</title>

      <para id="x_h31">Suppose we have a large file to compress and write to
	disk, but we want to handle a user's input quickly enough that
	they will perceive our program as responding immediately.  If
	we use <function>forkIO</function> to write the file out in a
	separate thread, we can do both simultaneously.</para>

      &Compressor.hs:module;

      <para id="x_i31">Because we're using lazy <type>ByteString</type> I/O here,
	all we really do in the main thread is open the file.  The
	actual reading occurs on demand in the other thread.</para>

      <para id="x_j31">The use of <code>handle print</code> gives us a cheap
	way to print an error message if the user enters the name of a
	file that does not exist.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Simple communication between threads</title>

    <para id="x_k31">The simplest way to share information between two threads is
      to let them both use a variable.  In our file compression
      example, the <function>main</function> thread shares both the
      name of a file and its contents with the other thread.  Because
      Haskell data is immutable by default, this poses no risks:
      neither thread can modify the other's view of the file's name or
      contents.</para>

    <para id="x_l31">We often need to have threads actively communicate with each
      other. For example, &GHC; does not provide a way for one thread
      to find out whether another is still executing, has completed,
      or has crashed<footnote>
	<para id="x_m31">As we will show later, &GHC; threads are extraordinarily
	  lightweight.  If the runtime were to provide a way to check
	  the status of every thread, the overhead of every thread
	  would increase, even if this information were never
	  used.</para>
      </footnote>.  However, it provides a <emphasis>synchronizing
	variable</emphasis> type, the <type>MVar</type>, which we can
      use to create this capability for ourselves.</para>

    <para id="x_n31">An <type>MVar</type> acts like a single-element box: it can
      be either full or empty.  We can put something into the box,
      making it full, or take something out, making it empty.</para>

    &mvar.ghci:MVar;

    <para id="x_o31">If we try to put a value into an <type>MVar</type> that is
      already full, our thread is put to sleep until another thread
      takes the value out.  Similarly, if we try to take a value from
      an empty <type>MVar</type>, our thread is put to sleep until
      some other thread puts a value in.</para>

    &MVarExample.hs:communicate;

    <para id="x_p31">The <function>newEmptyMVar</function> function has a
      descriptive name.  To create an <type>MVar</type> that starts
      out non-empty, we'd use <function>newMVar</function>.</para>

    &mvar.ghci:new;

    <para id="x_q31">Let's run our example in &ghci;.</para>

    &mvar.ghci:communicate;

    <para id="x_r31">If you're coming from a background of concurrent programming
      in a traditional language, you can think of an <type>MVar</type>
      as being useful for two familiar purposes.</para>

    <itemizedlist>
      <listitem>
	<para id="x_s31">Sending a message from one thread to another, e.g. a
	  notification.</para>
      </listitem>
      <listitem>
	<para id="x_t31">Providing <emphasis>mutual exclusion</emphasis> for a
	  piece of mutable data that is shared among threads.  We put
	  the data into the <type>MVar</type> when it is not being
	  used by any thread, and one thread takes it out temporarily
	  to read or modify it.</para>
      </listitem>
    </itemizedlist>

  </sect1>

  <sect1>
    <title>The main thread and waiting for other threads</title>

    <para id="x_u31">&GHC;'s runtime system treats the program's original thread
      of control differently from other threads.  When this thread
      finishes executing, the runtime system considers the program as
      a whole to have completed.  If any other threads are executing
      at the time, they are terminated.</para>

    <para id="x_v31">As a result, when we have long-running threads that must not
      be killed, we must make special arrangements to ensure that the
      main thread doesn't complete until the others do.  Let's develop
      a small library that makes this easy to do.</para>

    &NiceFork.hs:header;

    <para id="x_w31">We keep our <type>ThreadManager</type> type abstract using
      the usual recipe: we wrap it in a &newtype;, and prevent clients
      from creating values of this type.  Among our module's exports,
      we list the type constructor and the <type>IO</type> action that
      constructs a manager, but we do not export the data
      constructor.</para>

    &NiceFork.hs:module;

    <para id="x_x31">For the implementation of <type>ThreadManager</type>, we
      maintain a map from thread ID to thread state.  We'll refer to
      this as the <emphasis>thread map</emphasis>.</para>

    &NiceFork.hs:ThreadManager;

    <para id="x_y31">We have two levels of <type>MVar</type> use here.  We keep
      the <type>Map</type> in an <type>MVar</type>.  This lets us
      <quote>modify</quote> the map by replacing it with a new
      version.  We also ensure that any thread that uses the
      <type>Map</type> will see a consistent view of it.</para>

    <para id="x_z31">For each thread that we manage, we maintain an
      <type>MVar</type>.  A per-thread <type>MVar</type> starts off
      empty, which indicates that the thread is executing.  When the
      thread finishes or is killed by an uncaught exception, we put
      this information into the <type>MVar</type>.</para>

    <para id="x_A41">To create a thread and watch its status, we must perform a
      little bit of book-keeping.</para>

    &NiceFork.hs:forkManaged;

    <sect2>
      <title>Safely modifying an MVar</title>

      <para id="x_B41">The <function>modifyMVar</function> function that we used
	in <function>forkManaged</function> above is very useful: it's
	a safe combination of <function>takeMVar</function> and
	<function>putMVar</function>.</para>

      &mvar.ghci:modifyMVar;

      <para id="x_C41">It takes the value from an <type>MVar</type>, and passes
	it to a function.  This function can both generate a new value
	and return a result.  If the function throws an exception,
	<function>modifyMVar</function> puts the original value back
	into the <type>MVar</type>, otherwise it puts the new value
	in. It returns the other element of the function as its own
	result.</para>

      <para id="x_D41">When we use <function>modifyMVar</function> instead of
	manually managing an <type>MVar</type> with
	<function>takeMVar</function> and
	<function>putMVar</function>, we avoid two common kinds of
	concurrency bug.</para>

      <itemizedlist>
	<listitem>
	  <para id="x_E41">Forgetting to put a value back into an
	    <type>MVar</type>.  This can result in
	    <emphasis>deadlock</emphasis>, in which some thread waits
	    forever on an <type>MVar</type> that will never have a
	    value put into it.</para>
	</listitem>
	<listitem>
	  <para id="x_F41">Failure to account for the possibility that an
	    exception might be thrown, disrupting the flow of a piece
	    of code. This can result in a call to
	    <function>putMVar</function> that
	    <emphasis>should</emphasis> occur not actually happening,
	    again leading to deadlock.</para>
	</listitem>
      </itemizedlist>
    
      <para id="x_G41">Because of these nice safety properties, it's wise to use
	<function>modifyMVar</function> whenever possible.</para>

    </sect2>

    <sect2>
      <title>Safe resource management: a good idea, and easy
	besides</title>

      <para id="x_H41">We can the take the pattern that
	<function>modifyMVar</function> follows, and apply it to many
	other resource management situations.  Here are the steps of
	the pattern.</para>

      <orderedlist>
	<listitem>
	  <para id="x_I41">Acquire a resource.</para>
	</listitem>
	<listitem>
	  <para id="x_J41">Pass the resource to a function that will do something
	    with it.</para>
	</listitem>
	<listitem>
	  <para id="x_K41">Always release the resource, even if the function
	    throws an exception. If that occurs, rethrow the exception
	    so it can be caught by application code.</para>
	</listitem>
      </orderedlist>

      <para id="x_L41">Safety aside, this approach has another benefit: it can
	make our code shorter and easier to follow.  As we can see
	from looking at <function>forkManaged</function> above,
	Haskell's lightweight syntax for anonymous functions makes
	this style of coding visually unobtrusive.</para>

      <para id="x_M41">Here's the definition of <function>modifyMVar</function>,
	so that you can see a specific form of this pattern.</para>

      &ModifyMVar.hs:modifyMVar;

      <para id="x_N41">You should easily be able to adapt this to your particular
	needs, whether you're working with network connections,
	database handles, or data managed by a C library.</para>
    </sect2>

    <sect2>
      <title>Finding the status of a thread</title>

      <para id="x_O41">Our <function>getStatus</function> function tells us the
	current state of a thread.  If the thread is no longer managed
	(or was never managed in the first place), it returns
	<code>Nothing</code>.</para>

      &NiceFork.hs:getStatus;

      <para id="x_P41">If the thread is still running, it returns <code>Just
	  Running</code>. Otherwise, it indicates why the thread
	terminated, <emphasis>and</emphasis> stops managing the
	thread.</para>

      <para id="x_Q41">If the <function>tryTakeMVar</function> function finds
	that the <type>MVar</type> is empty, it returns
	<code>Nothing</code> immediately instead of blocking.</para>

      &mvar.ghci:tryTakeMVar;

      <para id="x_R41">Otherwise, it extracts the value from the
	<type>MVar</type> as usual.</para>

      <para id="x_S41">The <function>waitFor</function> function behaves
	similarly, but instead of returning immediately, it blocks
	until the given thread terminates before returning.</para>

      &NiceFork.hs:waitFor;

      <para id="x_T41">It first extracts the <type>MVar</type> that holds the
	thread's state, if it exists.  The <type>Map</type> type's
	<function>updateLookupWithKey</function> function is useful:
	it combines looking up a key with modifying or removing the
	value.</para>

      &niceFork.ghci:updateLookupWithKey;

      <para id="x_U41">In this case, we want to always remove the
	<type>MVar</type> holding the thread's state if it is present,
	so that our thread manager will no longer be managing the
	thread.  If there was a value to extract, we take the thread's
	exit status from the <type>MVar</type> and return it.</para>

      <para id="x_V41">Our final useful function simply waits for all currently
	managed threads to complete, and ignores their exit
	statuses.</para>

      &NiceFork.hs:waitAll;
    </sect2>

    <sect2>
      <title>Writing tighter code</title>

      <para id="x_W41">Our definition of <function>waitFor</function> above is a
	little unsatisfactory, because we're performing more or less
	the same case analysis in two places: inside the function
	called by <function>modifyMVar</function>, and again on its
	return value.</para>

      <para id="x_X41">Sure enough, we can apply a function that we came across
	earlier to eliminate this duplication.  The function in
	question is <function>join</function>, from the
	<code>Control.Monad</code> module.</para>

      &niceFork.ghci:join;

      <para id="x_Y41">The trick here is to see that we can get rid of the second
	&case; expression by having the first one return the
	<type>IO</type> action that we should perform once we return
	from <function>modifyMVar</function>.  We'll use
	<function>join</function> to execute the action.</para>

      &NiceFork.hs:waitFor2;

      <para id="x_Z41">This is an interesting idea: we can create a monadic
	function or action in pure code, then pass it around until we
	end up in a monad where we can use it.  This can be a nimble
	way to write code, once we develop an eye for when it makes
	sense.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Communicating over channels</title>

    <para id="x_a41">For one-shot communications between threads, an
      <type>MVar</type> is perfectly good.  Another type,
      <type>Chan</type>, provides a one-way communication channel.
      Here is a simple example of its use.</para>

    &Chan.hs:chanExample;

    <para id="x_b41">If a <type>Chan</type> is empty,
      <function>readChan</function> blocks until there is a value to
      read.  The <function>writeChan</function> function never blocks:
      it writes a new value into a <type>Chan</type>
      immediately.</para>
  </sect1>

  <sect1>
    <title>Useful things to know about</title>

    <sect2 id="concurrent.useful.nonstrict">
      <title>MVar and Chan are non-strict</title>

      <para id="x_c41">Like most Haskell container types, both <type>MVar</type>
	and <type>Chan</type> are non-strict: neither evaluates its
	contents.  We mention this not because it's a problem, but
	because it's a common blind spot: people tend to assume that
	these types are strict, perhaps because they're used in the
	<type>IO</type> monad.</para>

      <para id="x_d41">As for other container types, the upshot of a mistaken
	guess about the strictness of an <type>MVar</type> or
	<type>Chan</type> type is often a space or performance leak.
	Here's a plausible scenario to consider.</para>

      <para id="x_e41">We fork off a thread to perform some expensive computation
	on another core.</para>

      &Expensive.hs:notQuiteRight;

      <para id="x_f41">It <emphasis>seems</emphasis> to do something, and puts
	its result back into the <type>MVar</type>.</para>

      &Expensive.hs:expensiveComputation;

      <para id="x_g41">When we take the result from the <type>MVar</type> in the
	parent thread and attempt to do something with it, our thread
	starts computing furiously, because we never forced the
	computation to actually occur in the other thread!</para>

      <para id="x_h41">As usual, the solution is straightforward, once we know
	there's a potential for a problem: we add strictness to the
	forked thread, to ensure that the computation occurs there.
	This strictness is best added in one place, to avoid the
	possibility that we might forget to add it.</para>

      &ModifyMVarStrict.hs:modifyMVar_strict;

      <tip>
	<title>It's always worth checking Hackage</title>

	<para id="x_i41">In the Hackage package database, you will find a
	  library, <code>strict-concurrency</code>, that provides
	  strict versions of the <type>MVar</type> and
	  <type>Chan</type> types.</para>
      </tip>

      <para>The <code>!</code> pattern above is simple to use, but it
	is not always sufficient to ensure that our data is
	evaluated.  For a more complete approach, see <xref
	linkend="concurrent.strategies"/> below.</para>
    </sect2>

    <sect2>
      <title>Chan is unbounded</title>

      <para id="x_j41">Because <function>writeChan</function> always succeeds
	immediately, there is a potential risk to using a
	<type>Chan</type>.  If one thread writes to a
	<type>Chan</type> more often than another thread reads from
	it, the <type>Chan</type> will grow in an unchecked manner:
	unread messages will pile up as the reader falls further and
	further behind.</para>
    </sect2>
  </sect1>

  <sect1 id="concurrent.hard">
    <title>Shared-state concurrency is still hard</title>

    <para id="x_k41">Although Haskell has different primitives for sharing data
      between threads than other languages, it still suffers from the
      same fundamental problem: writing correct concurrent programs is
      fiendishly difficult.  Indeed, several pitfalls of concurrent
      programming in other languages apply equally to Haskell.  Two of
      the better known problems are <emphasis>deadlock</emphasis> and
      <emphasis>starvation</emphasis>.</para>

    <sect2>
      <title>Deadlock</title>

      <para id="x_l41">In a <emphasis>deadlock</emphasis> situation, two or more
	threads get stuck forever in a clash over access to shared
	resources. One classic way to make a multithreaded program
	deadlock is to forget the order in which we must acquire
	locks.  This kind of bug is so common, it has a name:
	<emphasis>lock order inversion</emphasis>. While Haskell
	doesn't provide locks, the <type>MVar</type> type is prone to
	the order inversion problem. Here's a simple example.</para>

      &LockHierarchy.hs:nestedModification;

      <para id="x_m41">If we run this in &ghci;, it will usually&emdash;but not
	always&emdash;print nothing, indicating that both threads have
	gotten stuck.</para>

      <para id="x_n41">The problem with the
	<function>nestedModification</function> function is easy to
	spot.  In the first thread, we take the <type>MVar</type>
	<varname>a</varname>, then <varname>b</varname>.  In the
	second, we take <varname>b</varname>, then
	<varname>a</varname>.  If the first thread succeeds in taking
	<varname>a</varname> and the second takes
	<varname>b</varname>, both threads will block: each tries to
	take an <type>MVar</type> that the other has already emptied,
	so neither can make progress.</para>

      <para id="x_o41">Across languages, the usual way to solve an order
	inversion problem is to always follow a consistent order when
	acquiring resources. Since this approach requires manual
	adherence to a coding convention, it is easy to miss in
	practice.</para>

      <para id="x_p41">To make matters more complicated, these kinds of inversion
	problems can be difficult to spot in real code.  The taking of
	<type>MVar</type>s is often spread across several functions in
	different files, making visual inspection more tricky. Worse,
	these problems are often <emphasis>intermittent</emphasis>,
	which makes them tough to even reproduce, never mind isolate
	and fix.</para>
    </sect2>

    <sect2>
      <title>Starvation</title>

      <para id="x_q41">Concurrent software is also prone to
	<emphasis>starvation</emphasis>, in which one thread
	<quote>hogs</quote> a shared resource, preventing another from
	using it.  It's easy to imagine how this might occur: one
	thread calls <function>modifyMVar</function> with a body that
	executes for 100 milliseconds, while another calls
	<function>modifyMVar</function> on the same <type>MVar</type>
	with a body that executes for 1 millisecond.  The second
	thread cannot make progress until the first puts a value back
	into the <type>MVar</type>.</para>

      <para id="x_r41">The non-strict nature of the <type>MVar</type> type can
	either exacerbate or cause a starvation problem.  If we put a
	thunk into an <type>MVar</type> that will be expensive to
	evaluate, and take it out of the <type>MVar</type> in a thread
	that otherwise looks like it <emphasis>ought</emphasis> to be
	cheap, that thread could suddenly become computationally
	expensive if it has to evaluate the thunk.  This makes the
	advice we gave in <xref linkend="concurrent.useful.nonstrict"/>
	particularly relevant.</para>
    </sect2>

    <sect2>
      <title>Is there any hope?</title>

      <para id="x_s41">Fortunately, the APIs for concurrency that we have covered
	here are by no means the end of the story.  A more recent
	addition to Haskell, Software Transactional Memory, is both
	easier and safer to work with.  We will discuss it in chapter
	<xref linkend="stm"/>.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Exercises</title>

    <qandaset defaultlabel="number">
      <qandaentry>
	<question>
	  <para id="x_t41">The <type>Chan</type> type is implemented using
	    <type>MVar</type>s.  Use <type>MVar</type>s to develop a
	    <type>BoundedChan</type> library.</para>
	</question>
      </qandaentry>

      <qandaentry>
	<question>
	  <para id="x_u41">Your <function>newBoundedChan</function> function
	    should accept an <type>Int</type> parameter, limiting the
	    number of unread items that can be present in a
	    <type>BoundedChan</type> at once.</para>
	</question>
      </qandaentry>

      <qandaentry>
	<question>
	  <para id="x_v41">If this limit is hit, a call to your
	    <function>writeBoundedChan</function> function must block
	    until a reader uses <function>readBoundedChan</function>
	    to consume a value.</para>
	</question>
      </qandaentry>

      <qandaentry>
	<question>
	  <para id="x_w41">Although we've already mentioned the existence of the
	    <code>strict-concurrency</code> package in the Hackage
	    repository, try developing your own, as a wrapper around
	    the built-in <type>MVar</type> type.  Following classic
	    Haskell practice, make your library type safe, so that
	    users cannot accidentally mix uses of strict and
	    non-strict <type>MVar</type>s.</para>
	</question>
      </qandaentry>
    </qandaset>
  </sect1>

  <sect1>
    <title>Using multiple cores with GHC</title>

    <para id="x_x41">By default, &GHC; generates programs that use just one core,
      even when we write explicitly concurrent code.  To use multiple
      cores, we must explicitly choose to do so.  We make this choice
      at <emphasis>link time</emphasis>, when we are generating an
      executable program.</para>

    <itemizedlist>
      <listitem>
	<para id="x_y41">The <quote>non-threaded</quote> runtime library runs all
	  Haskell threads in a single operating system thread.  This
	  runtime is highly efficient for creating threads and passing
	  data around in <type>MVar</type>s.</para>
      </listitem>
      <listitem>
	<para id="x_z41">The <quote>threaded</quote> runtime library uses
	  multiple operating system threads to run Haskell threads.
	  It has somewhat more overhead for creating threads and using
	  <type>MVar</type>s.</para>
      </listitem>
    </itemizedlist>

    <para id="x_A51">If we pass the <option>-threaded</option> option to the
      compiler, it will link our program against the threaded runtime
      library.  We do not need to use <option>-threaded</option> when
      we are compiling libraries or source files, only when we are
      finally generating an executable.</para>

    <para id="x_B51">Even when we select the threaded runtime for our program,
      it will still default to using only one core when we run it.  We
      must explicitly tell the runtime how many cores to use.</para>

    <sect2>
      <title>Runtime options</title>

      <para id="x_C51">We can pass options to &GHC;'s runtime system on the
	command line of our program.  Before handing control to our
	code, the runtime scans the program's arguments for the
	special command line option <option>+RTS</option>.  It
	interprets everything that follows, until the special option
	<option>-RTS</option>, as an option for the runtime system,
	not our program.  It hides all of these options from our code.
	When we use the <code>System.Environment</code> module's
	<function>getArgs</function>  function to obtain our command
	line arguments, we will not find any runtime options in the
	list.</para>

      <para id="x_D51">The threaded runtime accepts an option
	<option>-N</option><footnote>
	  <para id="x_E51">The non-threaded runtime does not understand this
	    option, and will reject it with an error message.</para>
	</footnote>.  This takes one argument, which specifies the
	number of cores that &GHC;'s runtime system should use.  The
	option parser is picky: there must be no spaces between
	<option>-N</option> and the number that follows it.  The option
	<option>-N4</option> is acceptable, but <option>-N 4</option>
	is not.</para>
    </sect2>

    <sect2>
      <title>Finding the number of available cores from Haskell</title>

      <para id="x_F51">The module <code>GHC.Conc</code> exports a variable,
	<varname>numCapabilities</varname>, that tells us how many
	cores the runtime system has been given with the
	<option>-N</option> RTS option.</para>

      &NumCapabilities.hs:main;

      <para id="x_G51">If we compile and run the above program, we can see that
	the options to the runtime system are not visible to the
	program, but that it can see how many cores it can run
	on.</para>

      <screen><prompt>$</prompt> <userinput>ghc -c NumCapabilities.hs</userinput>
<prompt>$</prompt> <userinput>ghc -threaded -o NumCapabilities NumCapabilities.o</userinput>
<prompt>$</prompt> <userinput>./NumCapabilities +RTS -N4 -RTS foo</userinput>
command line arguments: ["foo"]
number of cores: 4</screen>
    </sect2>

    <sect2>
      <title>Choosing the right runtime</title>

      <para id="x_H51">The decision of which runtime to use is not completely
	clear cut.  While the threaded runtime can use multiple cores,
	it has a cost: threads and sharing data between them are more
	expensive than with the non-threaded runtime.</para>

      <para id="x_I51">Furthermore, the garbage collector used by &GHC; as of
	version 6.8.3 is single threaded: it pauses all other threads
	while it runs, and executes on one core.  This limits the
	performance improvement we can hope to see from using multiple
	cores<footnote>
	  <para>As we write this book, the garbage collector is being
	    retooled to use multiple cores, but we cannot yet predict
	    its future effect.</para>
	</footnote>.</para>

      <para id="x_J51">In many real world concurrent programs, an individual
	thread will spend most of its time waiting for a network
	request or response. In these cases, if a single Haskell
	program serves tens of thousands of concurrent clients, the
	lower overhead of the non-threaded runtime may be helpful. For
	example, instead of having a single server program use the
	threaded runtime on four cores, we might see better
	performance if we design our server so that we can run four
	copies of it simultaneously, and use the non-threaded
	runtime.</para>

      <para id="x_K51">Our purpose here is not to dissuade you from
	using the threaded runtime.  It is not much more expensive
	than the non-threaded runtime: threads remain amazingly cheap
	compared to the runtimes of most other programming languages.
	We merely want to make it clear that switching to the threaded
	runtime will not necessarily result in an automatic
	win.</para>
    </sect2>
  </sect1>

  <sect1>
    <title>Parallel programming in Haskell</title>

    <para id="x_L51">We will now switch our focus to parallel programming. For
      many computationally expensive problems, we could calculate a
      result more quickly if we could divide up the solution, and
      evaluate it on many cores at once. Computers with multiple cores
      are already ubiquitous, but few programs can take advantage of
      the computing power of even a modern laptop.</para>

    <para id="x_M51">In large part, this is because parallel programming is
      traditionally seen as very difficult.  In a typical programming
      language, we would use the same libraries and constructs that we
      apply to concurrent programs to develop a parallel program. This
      forces us to contend with the familiar problems of deadlocks,
      race conditions, starvation, and sheer complexity.</para>

    <para id="x_N51">While we could certainly use Haskell's concurrency features
      to develop parallel code, there is a much simpler approach
      available to us.  We can take a normal Haskell function, apply a
      few simple transformations to it, and have it evaluated in
      parallel.</para>

    <sect2>
      <title>Normal form and head normal form</title>

      <para>The familiar <function>seq</function> function evaluates
	an expression to what we call <emphasis>head normal
	  form</emphasis> (abbreviated HNF).  It stops once it reaches
	the outermost constructor (the <quote>head</quote>). This is
	distinct from <emphasis>normal form</emphasis> (NF), in which
	an expression is completely evaluated.</para>

      <para>You will also hear Haskell programmers refer to
	<emphasis>weak</emphasis> head normal form (WHNF).  For normal
	data, weak head normal form is the same as head normal form.
	The difference only arises for functions, and is too abstruse
	to concern us here.</para>
    </sect2>

    <sect2>
      <title>Sequential sorting</title>

      <para id="x_O51">Here is a normal Haskell function that sorts a list using
	a divide-and-conquer approach.</para>

    &Sorting.hs:sort;

      <para id="x_P51">This function is inspired by the well-known Quicksort
	algorithm, and it is a classic among Haskell programmers: it
	is often presented as a one-liner early in a Haskell tutorial,
	to tease the reader with an example of Haskell's
	expressiveness. Here, we've split the code over a few lines,
	to make it easier to compare the serial and parallel
	versions.</para>

      <para id="x_Q51">Here is a very brief description of how
	<function>sort</function> operates.</para>

      <orderedlist>
	<listitem>
	  <para id="x_R51">It chooses an element from the list.  This is called
	    the <emphasis>pivot</emphasis>.  Any element would do as
	    the pivot; the first is merely the easiest to pattern
	    match on.</para>
	</listitem>
	<listitem>
	  <para id="x_S51">It creates a sublist of all elements less than the
	    pivot, and recursively sorts them.</para>
	</listitem>
	<listitem>
	  <para id="x_T51">It creates a sublist of all elements greater than or
	    equal to the pivot, and recursively sorts them.</para>
	</listitem>
	<listitem>
	  <para id="x_U51">It appends the two sorted sublists.</para>
	</listitem>
      </orderedlist>
    </sect2>

    <sect2>
      <title>Transforming our code into parallel code</title>

      <para id="x_V51">The parallel version of the function is only a little more
	complicated than the initial version.</para>

      &Sorting.hs:parSort;

      <para id="x_W51">We have barely perturbed the code: all we have added are
	three functions, <function>par</function>,
	<function>pseq</function>, and
	<function>force</function>.</para>

      <para id="x_X51">The <function>par</function> function is
	provided by the <code>Control.Parallel</code> module.  It
	serves a similar purpose to <function>seq</function>: it
	evaluates its left argument to weak head normal form, and
	returns its right. As its name suggests,
	<function>par</function> can evaluate its left argument in
	parallel with whatever other evaluations are occurring.</para>

      <para id="x_Y51">As for <function>pseq</function>, it is similar
	to <function>seq</function>: it evaluates the expression on
	the left to WHNF before returning the expression on the right.
	The difference between the two is subtle, but important for
	parallel programs: the compiler does not
	<emphasis>promise</emphasis> to evaluate the left argument of
	<function>seq</function> if it can see that evaluating the
	right argument first would improve performance.  This
	flexibility is fine for a program executing on one core, but
	it is not strong enough for code running on multiple cores.
	In contrast, the compiler <emphasis>guarantees</emphasis> that
	<function>pseq</function> will evaluate its left argument
	before its right.</para>

      <para id="x_Z51">These changes to our code are remarkable for all the
	things we have <emphasis>not</emphasis> needed to say.</para>

      <itemizedlist>
	<listitem>
	  <para id="x_a51">How many cores to use.</para>
	</listitem>
	<listitem>
	  <para id="x_b51">What threads do to communicate with each other.</para>
	</listitem>
	<listitem>
	  <para id="x_c51">How to divide up work among the available cores.</para>
	</listitem>
	<listitem>
	  <para id="x_d51">Which data are shared between threads, and which are
	    private.</para>
	</listitem>
	<listitem>
	  <para id="x_e51">How to determine when all the participants are
	    finished.</para>
	</listitem>
      </itemizedlist>
    </sect2>

    <sect2>
      <title>Knowing what to evaluate in parallel</title>

      <para id="x_f51">The key to getting decent performance out of parallel
	Haskell code is to find meaningful chunks of work to perform
	in parallel.  Non-strict evaluation can get in the way of
	this, which is why we use the <function>force</function>
	function in our parallel sort.  To best explain what the
	<function>force</function> function is for, we will first look
	at a mistaken example.</para>

      &Sorting.hs:sillySort;

      <para id="x_g51">Take a look at the small changes in each use of
	<function>par</function>.  Instead of <code>force
	  lesser</code> and <code>force greater</code>, here we
	evaluate <code>lesser</code> and <code>greater</code>.</para>

      <para id="x_h51">Remember that evaluation to WHNF only computes enough of
	an expression to see its <emphasis>outermost</emphasis>
	constructor.  In this mistaken example, we evaluate each
	sorted sublist to WHNF.  Since the outermost constructor in
	each case is just a single list constructor, we are in fact
	only forcing the evaluation of the first element of each
	sorted sublist!  Every other element of each list remains
	unevaluated.  In other words, we do almost no useful work in
	parallel: our <function>sillySort</function> is nearly
	completely sequential.</para>

      <para id="x_i51">We avoid this with our <function>force</function> function
	by forcing the entire spine of a list to be evaluated before
	we give back a constructor.</para>

      &Sorting.hs:force;

      <para id="x_j51">Notice that we don't care what's in the list;
	  we walk down its spine to the end, then use
	  <function>pseq</function> once.  There is clearly no magic
	  involved here: we are just using our usual understanding of
	  Haskell's evaluation model.  And because we will be using
	  <function>force</function> on the left hand side of
	  <function>par</function> or <function>pseq</function>, we
	  don't need to return a meaningful value.</para>

	<para>Of course, in many cases we will need to force the
	  evaluation of individual elements of the list, too.  Below,
	  we will discuess a typeclass-based solution to this
	  problem.</para>
    </sect2>

    <sect2>
      <title>What promises does par make?</title>

      <para id="x_k51">The <function>par</function> function does not actually
	promise to evaluate an expression in parallel with another.
	Instead, it undertakes to do so if it <quote>makes
	  sense</quote>.  This wishy-washy non-promise is actually
	more useful than a guarantee to always evaluate an expression
	in parallel.  It gives the runtime system the freedom to act
	intelligently when it encounters a use of
	<function>par</function>.</para>

      <para id="x_l51">For instance, the runtime could decide that an expression
	is too cheap to be worth evaluating in parallel.  Or it might
	notice that all cores are currently busy, so that
	<quote>sparking</quote> a new parallel evaluation will lead to
	there being more runnable threads than there are cores
	available to execute them.</para>

      <para id="x_m51">This lax specification in turn affects how we write parallel
	code.  Since <function>par</function> may be somewhat
	intelligent at runtime, we can use it almost wherever we like,
	on the assumption that performance will not be bogged down by
	threads contending for busy cores.</para>
    </sect2>

    <sect2>
      <title>Running our code, and measuring performance</title>

      <para id="x_n51">To try our code out, let's save <function>sort</function>,
	<function>parSort</function>, and
	<function>parSort2</function> to a module named
	<filename>Sorting.hs</filename>.  We create a small driver
	program that we can use to time the performance of one of
	those sorting function.</para>

      &SortMain.hs:main;

      <para id="x_o51">For simplicity, we choose the sorting function to
	benchmark at compilation time, via the
	<varname>testFunction</varname> variable.</para>

      <para id="x_p51">Our program accepts a single optional command line
	argument, the length of the random list to generate.</para>

      <para id="x_q51">Non-strict evaluation can turn performance measurement and
	analysis into something of a minefield.  Here are some
	potential problems that we specifically work to avoid in our
	driver program.</para>

      <itemizedlist>
	<listitem>
	  <para id="x_r51"><emphasis>Measuring several things, when we think we
	      are looking at just one.</emphasis>  Haskell's default
	    pseudorandom number generator (PRNG) is slow, and the
	    <function>randoms</function> function generates random
	    numbers on demand.</para>

	  <para id="x_s51">Before we record our starting time, we force every
	    element of the input list to be evaluated, and we print
	    the length of the list: this ensures that we create all of
	    the random numbers that we will need in advance.</para>

	  <para id="x_t51">If we were to omit this step, we would interleave the
	    generation of random numbers with attempts to work with
	    them in parallel.  We would thus be measuring both the
	    cost of sorting the numbers and, less obviously, the cost
	    of generating them.</para>
	</listitem>

	<listitem>
	  <para id="x_u51"><emphasis>Invisible data dependencies.</emphasis> When
	    we generate the list of random numbers, simply printing
	    the length of the list would not perform enough
	    evaluation. This wouls evaluate the
	    <emphasis>spine</emphasis> of the list, but not its
	    elements.  The actual random numbers would not be
	    evaluated until the sort compares them.</para>

	  <para id="x_v51">This can have serious consequences for performance.
	    The value of a random number depends on the value of the
	    preceding random number in the list, but we have scattered
	    the list elements randomly among our processor cores.  If
	    we did not evaluate the list elements prior to sorting, we
	    would suffer a terrible <quote>ping pong</quote> effect:
	    not only would evaluation bounce from one core to another,
	    performance would suffer.</para>

	  <para id="x_w51">Try snipping out the application of
	    <function>force</function> from the body of
	    <function>main</function> above: you should find that the
	    parallel code can easily end up three times
	    <emphasis>slower</emphasis> than the non-parallel
	    code.</para>
	</listitem>

	<listitem>
	  <para id="x_x51"><emphasis>Benchmarking a thunk, when we believe that
	      the code is performing meaningful work.</emphasis>  To
	    force the sort to take place, we print the length of the
	    result list before we record the ending time. Without
	    <function>putStrLn</function> demanding the length of the
	    list in order to print it, the sort would not occur at
	    all.</para>
	</listitem>
      </itemizedlist>

      <para id="x_y51">When we build the program, we enable optimization and
	&GHC;'s threaded runtime.</para>

      <screen><prompt>$</prompt> <userinput>ghc -threaded -O2 --make SortMain</userinput>
[1 of 2] Compiling Sorting          ( Sorting.hs, Sorting.o )
[2 of 2] Compiling Main             ( SortMain.hs, SortMain.o )
Linking SortMain ...
</screen>

      <para id="x_z51">When we run the program, we must tell &GHC;'s
	runtime how many cores to use.  Initially, we try the original
	<function>sort</function>, to establish a performance
	baseline.</para>

      <screen><prompt>$</prompt> <userinput>./Sorting +RTS -N1 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.178941s elapsed.
</screen>

      <para id="x_A61">Enabling a second core ought to have no effect on
	performance.</para>

      <screen><prompt>$</prompt> <userinput>./Sorting +RTS -N2 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.259869s elapsed.
</screen>

      <para id="x_B61">If we recompile and test the performance of
	<function>parSort</function>, the results are less than
	stellar.</para>

      <screen><prompt>$</prompt> <userinput>./Sorting +RTS -N1 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.915818s elapsed.
<prompt>$</prompt> <userinput>./Sorting +RTS -N2 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
4.029781s elapsed.
</screen>

      <para id="x_C61">We have gained nothing in performance.  It seems that this
	could be due to one of two factors: either
	<function>par</function> is intrinsically expensive, or we are
	using it too much. To help us to distinguish between the two
	possibilities, here is a sort is identical to
	<function>parSort</function>, but it uses
	<function>pseq</function> instead of
	<function>par</function>.</para>
      
      &Sorting.hs:seqSort;

      <para id="x_D61">We also drop the use of <function>force</function>, so
	compared to our original <function>sort</function>, we should
	only be measuring the cost of using <function>pseq</function>.
	What effect does <function>pseq</function> alone have on
	performance?</para>

      <screen><prompt>$</prompt> <userinput>./Sorting +RTS -N1 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
3.848295s elapsed.
</screen>

      <para id="x_E61">This suggests that <function>par</function> and
	<function>pseq</function> have similar costs.  What can we do
	to improve performance?</para>
    </sect2>

    <sect2>
      <title>Tuning for performance</title>

      <para id="x_F61">In our <function>parSort</function>, we perform twice as
	many applications of <function>par</function> as there are
	elements to sort. While <function>par</function> is
	<emphasis>cheap</emphasis>, as we have seen, it is not
	<emphasis>free</emphasis>.  When we recursively apply
	<function>parSort</function>, we eventually apply
	<function>par</function> to individual list elements.  At this
	fine granularity, the cost of using <function>par</function>
	outweighs any possible usefulness.  To reduce this effect, we
	switch to our non-parallel <function>sort</function> after
	passing some threshold.</para>

      &Sorting.hs:parSort2;
      
      <para id="x_G61">Here, we stop recursing and sparking new parallel
	evaluations at a controllable depth.  If we knew the size of
	the data we were dealing with, we could stop subdividing and
	switch to the non-parallel code once we reached a sufficiently
	small amount of remaining work.</para>

      <screen><prompt>$</prompt> <userinput>./Sorting +RTS -N2 -RTS 700000</userinput>
We have 700000 elements to sort.
Sorted all 700000 elements.
2.947872s elapsed.
</screen>

      <para id="x_H61">On a dual core system, this gives us roughly a 25%
	speedup.  This is not a huge number, but consider the number
	of changes we had to make in return for this performance
	improvement: just a few annotations.</para>

      <para id="x_I61">This sorting function is particularly resistant to good
	parallel performance.  The amount of memory allocation it
	performs forces the garbage collector to run frequently.  We
	can see the effect by running our program with the
	<option>-sstderr</option> RTS option, which prints garbage
	collection statistics to the screen.  This indicates that our
	program spends roughly 40% of its time collecting garbage.
	Since the garbage collector in &GHC; 6.8 stops all threads and
	runs on a single core, it acts as a bottleneck.</para>

      <para id="x_J61">You can expect more impressive performance improvements
	from less allocation-heavy code when you use
	<function>par</function> annotations.  We have seen some
	simple numerical benchmarks run 1.8 times faster on a dual
	core system than with a single core.  As we write this book, a
	parallel garbage collector is under development for &GHC;,
	which should help considerably with the performance of
	allocation-heavy code on multicore systems.</para>

      <warning>
	<title>Beware a GC bug in GHC 6.8.2</title>

	<para id="x_K61">The garbage collector in release 6.8.2 of
	    &GHC; has a bug that can cause programs using
	    <function>par</function> to crash.  If you want to use
	    <function>par</function> and you are using 6.8.2, we
	    suggest upgrading to at least 6.8.3.</para>
      </warning>
    </sect2>

    <sect2>
      <title>Exercises</title>

      <qandaset defaultlabel="number">
	<qandaentry>
	  <question>
	    <para id="x_L61">It can be difficult to determine when to switch from
	      <function>parSort2</function> to
	      <function>sort</function>.  An alternative approach to
	      the one we outline above would be to decide based on the
	      length of a sublist.  Rewrite
	      <function>parList2</function> so that it
	      switches to <function>sort</function> if the list
	      contains more than some number of elements.</para>
	  </question>
	</qandaentry>

	<qandaentry>
	  <question>
	    <para id="x_M61">Measure the performance of the length-based
	      approach, and compare with the depth approach.  Which
	      gives better performance results?</para>
	  </question>
	</qandaentry>
      </qandaset>
    </sect2>
  </sect1>

  <sect1>
    <title>Parallel strategies and MapReduce</title>

    <para id="x_N61">Within the programming community, one of the most famous
      software systems to credit functional programming for
      inspiration is Google's MapReduce infrastructure for parallel
      processing of bulk data.</para>

    <para id="x_O61">We can easily construct a greatly simplified, but still
      useful, Haskell equivalent.  To focus our attention, we will
      look at the processing of web server log files, which tend to be
	both huge and plentiful<footnote>
	  <para>The genesis of this idea comes from Tim Bray.</para>
	</footnote>.</para>

    <para id="x_P61">As an example, here is a log entry for a page visit recorded
      by the Apache web server.  The entry originally filled one line;
      we have split it across several lines to fit.</para>

<programlisting>201.49.94.87 - - [08/Jun/2008:07:04:20 -0500] "GET / HTTP/1.1"
200 2097 "http://en.wikipedia.org/wiki/Mercurial_(software)"
"Mozilla/5.0 (Windows; U; Windows XP 5.1; en-GB; rv:1.8.1.12)
Gecko/20080201 Firefox/2.0.0.12" 0 hgbook.red-bean.com</programlisting>


    <para id="x_Q61">While we could create a straightforward implementation
      without much effort, we will resist the temptation to dive in.
      If we think about solving a <emphasis>class</emphasis> of
      problems instead of a single one, we may end up with more widely
      applicable code.</para>

    <para id="x_R61">When we develop a parallel program, we are always faced with
      a few <quote>bad penny</quote> problems, which turn up no matter
      what the underlying programming language is.</para>

    <itemizedlist>
      <listitem>
	<para id="x_S61">Our algorithm quickly becomes obscured by the details of
	  partitioning and communication.  This makes it difficult to
	  understand code, which in turn makes modifying it
	  risky.</para>
      </listitem>

      <listitem>
	<para id="x_T61">Choosing a <quote>grain size</quote>&emdash;the smallest
	  unit of work parceled out to a core&emdash; can be
	  difficult. If the grain size is too small, cores spend so
	  much of their time on book-keeping that a parallel program
	  can easily become slower than a serial counterpart.  If the
	  grain size is too large, some cores may lie idle due to poor
	  load balancing.</para>
      </listitem>
    </itemizedlist>
    
    <sect2 id="concurrent.strategies">
      <title>Separating algorithm from evaluation</title>

      <para id="x_U61">In parallel Haskell code, the clutter that would arise
	from communication code in a traditional language is replaced
	with the clutter of <function>par</function> and
	<function>pseq</function> annotations.  As an example, this
	function operates similarly to <function>map</function>, but
	evaluates each element to weak head normal form (WHNF) in
	parallel as it goes.</para>

      &ParMap.hs:parallelMap;

      <para id="x_V61">The type <varname role="type">b</varname> might be a list,
	or some other type for which evaluation to WHNF doesn't do a
	useful amount of work.  We'd prefer not to have to write a
	special <function>parallelMap</function> for lists, and for
	every other type that needs special handling.</para>

      <para id="x_W61">To address this problem, we will begin by considering a
	simpler problem: how to force a value to be evaluated.  Here
	is a function that forces every element of a list to be
	evaluated to WHNF.</para>

      &ParMap.hs:forceList;

      <para id="x_X61">Our function performs no computation on the list. (In
	fact, from examining its type signature, we can tell that it
	<emphasis>cannot</emphasis> perform any computation, since it
	knows nothing about the elements of the list.)  Its only
	purpose is to ensure that the spine of the list is evaluated
	to head normal form. The only place that it makes any sense to
	apply this function is in the first argument of
	<function>seq</function> or <function>par</function>, for
	example as follows.</para>

      &ParMap.hs:stricterMap;

      <para id="x_Y61">This still leaves us with the elements of the list
	evaluated only to WHNF.  We address this by adding a function
	as parameter that can force an element to be evaluated more
	deeply.</para>

      &ParMap.hs:forceListAndElts;

      <para id="x_Z61">The <code>Control.Parallel.Strategies</code>
	  module generalizes this idea into something we can use as a
	  library.  It introduces the idea of an <emphasis>evaluation
	    strategy</emphasis>.</para>

      &Strat.hs:Strategy;

      <para id="x_a61">An evaluation strategy performs no computation; it simply
	ensures that a value is evaluated to some extent.  The
	simplest strategy is named <function>r0</function>, and does
	nothing at all.</para>

      &Strat.hs:r0;

      <para id="x_b61">Next is <function>rwhnf</function>, which evaluates a
	value to weak head normal form.</para>

      &Strat.hs:rwhnf;

      <para id="x_c61">To evaluate a value to normal form, the module provides a
	typeclass with a method named <function>rnf</function>.</para>

      &Strat.hs:NFData;

      <tip>
	<title>Remembering those names</title>

	<para id="x_d61">If the names of these functions and types are not
	  sticking in your head, look at them as acronyms.  The name
	  <function>rwhnf</function> expands to <quote>reduce to weak
	    head normal form</quote>; <type>NFData</type> becomes
	  <quote>normal form data</quote>; and so on.</para>
      </tip>

      <para id="x_e61">For the basic types, such as <type>Int</type>, weak head
	normal form and normal form are the same thing, which is why
	the <type>NFData</type> typeclass uses
	<function>rwhnf</function> as the default implementation of
	<function>rnf</function>. For many common types, the
	<code>Control.Parallel.Strategies</code> module provides
	instances of <type>NFData</type>.</para>

      &Strat.hs:instances;

      <para id="x_f61">From these examples, it should be clear how you might
	write an <type>NFData</type> instance for a type of your own.
	Your implementation of <function>rnf</function> must handle
	every constructor, and apply <function>rnf</function> to every
	field of a constructor.</para>
    </sect2>

    <sect2>
      <title>Separating algorithm from strategy</title>

      <para id="x_g61">From these strategy building blocks, we can construct more
	elaborate strategies.  Many are already provided by
	<code>Control.Parallel.Strategies</code>.  For instance,
	<function>parList</function> applies an evaluation strategy in
	parallel to every element of a list.</para>

      &Strat.hs:parList;

      <para id="x_h61">The module uses this to define a parallel
	<function>map</function> function.</para>

      &Strat.hs:parMap;

      <para id="x_i61">This is where the code becomes interesting.  On the left
	of <function>using</function>, we have a normal application of
	<function>map</function>.  On the right, we have an evaluation
	strategy.  The <function>using</function> combinator tells us
	how to apply a strategy to a value, allowing us to keep the
	code separate from how we plan to evaluate it.</para>

      &Strat.hs:using;

      <para id="x_j61">The <code>Control.Parallel.Strategies</code> module
	provides many other functions that provide fine control over
	evaluation.  For instance, <function>parZipWith</function>
	that applies <function>zipWith</function> in parallel, using
	an evaluation strategy.</para>

      &Strat.hs:vectorSum;
    </sect2>

    <sect2>
      <title>Writing a simple MapReduce definition</title>

      <para id="x_k61">We can quickly suggest a type for a
	<function>mapReduce</function> function by considering what it
	must do.  We need a <emphasis>map</emphasis> component, to
	which we will give the usual type <type>a -&gt; b</type>.  And
	we need a <emphasis>reduce</emphasis>; this term is a synonym
	for <emphasis>fold</emphasis>.  Rather than commit ourselves
	to using a specific kind of fold, we'll use a more general
	type, <type>[b] -&gt; c</type>.  This type lets us use a left
	or right fold, so we can choose the one that suits our data
	and processing needs.</para>

      <para id="x_l61">If we plug these types together, the complete type looks
	like this.</para>

      &MapReduce.hs:simpleMapReduce.type;

      <para id="x_m61">The code that goes with the type is extremely
	simple.</para>

      &MapReduce.hs:simpleMapReduce;
    </sect2>

    <sect2>
      <title>MapReduce and strategies</title>

      <para id="x_n61">Our definition of <function>simpleMapReduce</function> is
	too simple to really be interesting.  To make it useful, we
	want to be able to specify that some of the work should occur
	in parallel.  We'll achieve this using strategies, passing in
	a strategy for the map phase and one for the reduction phase.</para>

      &MapReduce.hs:mapReduce.type;

      <para id="x_o61">Both the type and the body of the function must grow a
	little in size to accommodate the strategy parameters.</para>

      &MapReduce.hs:mapReduce;

    </sect2>

    <sect2>
      <title>Sizing work appropriately</title>

      <para id="x_p61">To achieve decent performance, we must ensure that the
	work that we do per application of <function>par</function>
	substantially outweighs its book-keeping costs.  If we are
	processing a huge file, splitting it on line boundaries gives
	us far too little work compared to overhead.</para>

      <para id="x_q61">We will develop a way to process a file in larger chunks
	in a later section.  What should those chunks consist of?
	Because a web server log file ought to contain only ASCII
	text, we will see excellent performance with a lazy
	<type>ByteString</type>: this type is highly efficient, and
	consumes little memory when we stream it from a file.</para>

      &LineChunks.hs:withChunks;

      <para id="x_r61">We consume each chunk in parallel, taking careful
	advantage of lazy I/O to ensure that we can stream these
	chunks safely.</para>

      <sect3>
	<title>Mitigating the risks of lazy I/O</title>

	<para id="x_s61">Lazy I/O poses a few well known hazards that
	    we would like to avoid.</para>

	<itemizedlist>
	  <listitem>
	    <para id="x_t61">We may invisibly keep a file handle open for longer
	      than necessary, by not forcing the computation that
	      pulls data from it to be evaluated.  Since an operating
	      system will typically place a small, fixed limit on the
	      number of files we can have open at once, if we do not
	      address this risk, we can accidentally starve some other
	      part of our program of file handles.</para>
	  </listitem>
	  <listitem>
	    <para id="x_u61">If we do not explicitly close a file handle, the
	      garbage collector will automatically close it for us. It
	      may take a long time to notice that it should close the
	      file handle.  This poses the same starvation risk as
	      above.</para>
	  </listitem>
	  <listitem>
	    <para id="x_v61">We can avoid starvation by explicitly closing a file
	      handle. If we do so too early, though, we can cause a
	      lazy computation to fail if it expects to be able to
	      pull more data from a closed file handle.</para>
	  </listitem>
	</itemizedlist>

	<para id="x_w61">On top of these well-known risks, we cannot use a single
	  file handle to supply data to multiple threads.  A file
	  handle has a single <quote>seek pointer</quote> that tracks
	  the position from which it should be reading, but when we
	  want to read multiple chunks, each needs to consume data
	  from a different position in the file.</para>

	<para id="x_x61">With these ideas in mind, let's fill out the lazy I/O
	  picture.</para>

	&LineChunks.hs:chunkedRead;

	<para id="x_y61">We avoid the starvation problem by explicitly closing
	  file handles.  We allow multiple threads to read different
	  chunks at once by supplying each one with a distinct file
	  handle, all reading the same file.</para>

	<para id="x_z61">The final problem that we try to mitigate is that of a
	  lazy computation having a file handle closed behind its
	  back.  We use <function>rnf</function> to force all of our
	  processing to complete before we return from
	  <function>withChunks</function>. We can then close our file
	  handles explicitly, as they should no longer be read from.
	  If you must use lazy I/O in a program, it is often best to
	  <quote>firewall</quote> it like this, so that it cannot
	  cause problems in unexpected parts of your code.</para>

	<tip>
	  <title>Processing chunks via a fold</title>

	  <para id="x_A71">We can adapt the fold-with-early-termination technique
	    from <xref linkend="find.fold"/> to stream-based file
	    processing. While this requires more work than the lazy
	    I/O approach, it nicely avoids the above problems.</para>
	</tip>

      </sect3>
    </sect2>

    <sect2>
      <title>Efficiently finding line-aligned chunks</title>

      <para id="x_B71">Since a server log file is line-oriented, we need an
	efficient way to break a file into large chunks, while making
	sure that each chunk ends on a line boundary.  Since a chunk
	might be tens of megabytes in size, we don't want to scan all
	of the data in a chunk to determine where its final boundary
	should be.</para>

      <para id="x_C71">Our approach works whether we choose a fixed chunk size or
	a fixed number of chunks.  Here, we opt for the latter.  We
	begin by seeking to the approximate position of the end of a
	chunk, then scan forwards until we reach a newline character.
	We then start the next chunk after the newline, and repeat the
	procedure.</para>

      &LineChunks.hs:lineChunks;

      <para id="x_D71">The last chunk will end up a little shorter than its
	predecessors, but this difference will be insignificant in
	practice.</para>
    </sect2>

    <sect2>
      <title>Counting lines</title>

      <para id="x_E71">This simple example illustrates how to use the scaffolding
	we have built.</para>

      &LineCount.hs:countLines;

      <para id="x_F71">If we compile this program with <option>ghc -O2 --make
	  -threaded</option>, it should perform well after an initial
	run to <quote>warm</quote> the filesystem cache. On a dual
	core laptop, processing a log file 248 megabytes (1.1 million
	lines) in size, this program runs in 0.576 seconds using a
	single core, and 0.361 with two (using <option>+RTS
	  -N2</option>).</para>
    </sect2>

    <sect2>
      <title>Finding the most popular URLs</title>

      <para id="x_G71">In this example, we count the number of times each URL is
	accessed.  This example comes from <biblioref
	  linkend="bib.google08"/>, Google's original paper discussing
	MapReduce.  In the <emphasis>map</emphasis> phase, for each
	chunk, we create a <type>Map</type> from URL to the number of
	times it was accessed.  In the <emphasis>reduce</emphasis>
	phase, we union-merge these maps into one.</para>

      &CommonURLs.hs:countURLs;

      <para id="x_H71">To pick a URL out of a line of the log file, we use the
	bindings to the PCRE regular expression library that we
	developed in <xref linkend="ffi"/>.</para>

      <para id="x_I71">Our driver function prints the ten most popular URLs.  As
	with the line counting example, this program runs about 1.8
	times faster with two cores than with one, taking 1.7 seconds
	to process the a log file containing 1.1 million
	entries.</para>
    </sect2>

    <sect2>
      <title>Conclusions</title>

      <para id="x_J71">Given a problem that fits its model well, the MapReduce
	programming model lets us write <quote>casual</quote> parallel
	programs in Haskell with good performance, and  minimal
	additional effort.  We can easily extend the idea to use other
	data sources, such as collections of files, or data sourced
	over the network.</para>

      <para id="x_K71">In many cases, the performance bottleneck will be
	streaming data at a rate high enough to keep up with a core's
	processing capacity. For instance, if we try to use either of
	the above sample programs on a file that is not cached in
	memory or streamed from a high-bandwidth storage array, we
	will spend most of our time waiting for disk I/O, gaining no
	benefit from multiple cores.</para>
    </sect2>
  </sect1>
</chapter>

<!--
local variables: 
sgml-parent-document: ("00book.xml" "book" "chapter")
end:
-->
