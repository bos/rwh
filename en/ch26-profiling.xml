<!-- vim: set filetype=docbkxml shiftwidth=2 autoindent expandtab tw=77 : -->

<chapter id="profiling">
  <title>Profiling and optimisation</title>

<para id="x_qJ1">
Haskell is a high level language. A really high level language. We can spend
our days programming entirely in abstractions, in monoids, functors and
hylomorphisms, far removed from any particular hardware model of computation.
The language specification goes to great lengths to avoid prescribing any
particular evaluation model. These layers of abstraction let us treat Haskell
as a notation for computation itself, letting the programmer concentrate on the
essence of their problem without getting bogged down in low level
implementation decisions. We get to program in pure thought.
</para>

<para id="x_rJ1">
However, this is a book about real world programming, and in the real world,
code runs on stock hardware with limited resources. Our programs will have
time and space requirements that we may need to enforce. As such, we need a
good knowledge of how our program data is represented, the precise consequences
of using lazy or strict evaluation strategies, and techniques for analysing and
controlling space and time behaviour.
</para>

<para id="x_sJ1">
In this chapter we'll look at typical space and time problems a Haskell
programmer might encounter, and how to methodically analyse, understand and
address them. To do this we'll use time and space profiling, and collect
runtime statistics, as well as reasoning about how strict or lazy evaluation
strategies affect behaviour. We'll also look at the impact of compiler
optimisations on performance, and the use of advanced optimisation techniques
that become feasible in a purely functional language. So let's begin with a
classic challenge: squashing excessive memory usage in some inoccuous looking
code.
</para>

<sect1>
  <title>Profiling Haskell programs</title>

<!-- the heap -->

<para id="x_tJ1">
Let's consider the following list manipulating program, which naively computes
the mean of some large list of values. While only a program fragment (and
we'll stress that the particular algorithm we're implementing is irrelevant
here), it is representative of real code we might find in any Haskell
program: typically concise list manipulation code, and heavy use of standard
library functions. It also illustrates several common performance trouble
spots that can catch out the unwary: </para>

&A.hs:naive;

<para id="x_uJ1">
This program is very simple: we import functions for accessing the
system's environment (in particular, <function>getArgs</function>), and the
Haskell version of <function>printf</function>, for formatted text output.
The program then reads a numeric literal from the command line, using that to
build a list of floating point values, whose mean value we compute by
dividing the list sum by its length. The result is printed as a string. Let's
compile this source to native code (with optimisations on) and run it with
the standard <code>time</code> command to see how it performs:
</para>

<!-- flags to the runtime system -->

<screen>
<prompt>$ </prompt><userinput>ghc --make -O2 A.hs</userinput>
[1 of 1] Compiling Main             ( A.hs, A.o )
Linking A ...
<prompt>$ </prompt><userinput>time ./A 1e5</userinput>
50000.5
./A 1e5  0.05s user 0.01s system 102% cpu 0.059 total
<prompt>$ </prompt><userinput>time ./A 1e6</userinput>
500000.5
./A 1e6  0.26s user 0.04s system 99% cpu 0.298 total
<prompt>$ </prompt><userinput>time ./A 1e7</userinput>
5000000.5
./A 1e7  63.80s user 0.62s system 99% cpu 1:04.53 total
</screen>

<para id="x_vJ1">
It worked well for small numbers, but the program really started to struggle
with input size of ten million. From this alone we know something's not quite
right, but it's unclear what resources are being used. Let's investigate, and
see what we can find out about this program's behaviour.
</para>

<sect2>
  <title>Collecting runtime statistics</title>
  
<para id="x_wJ1">  
To get access to that kind of information, GHC lets us pass flags directly to
the Haskell runtime, using the special <code>+RTS</code> and
<code>-RTS</code> flags to delimit arguments reserved for the runtime system.
The application itself won't see those flags as they're immediately consumed 
by the Haskell runtime system.
</para>

<para id="x_xJ1">  
In particular, we can ask the runtime system to gather memory and garbage
collector performance numbers with the <code>-s</code> flag (as well as
controlling the number of OS threads with <code>-N</code>, or tweaking the
stack and heaps sizes). We'll also use runtime flags to enabling different
varieties of profiling. The complete set of runtime flags the Haskell
runtime accepts is documented in the
<ulink
url="http://www.haskell.org/ghc/docs/latest/html/users_guide/">GHC User's Guide</ulink>:
</para>

<para id="x_yJ1">
So let's run the program with statistic reporting enabled, via <code>+RTS -sstderr</code>,
yielding this result:
</para>

<screen>
<prompt>$ </prompt><userinput>./A 1e7 +RTS -sstderr</userinput>
./A 1e7 +RTS -sstderr 
5000000.5
1,689,133,824 bytes allocated in the heap
697,882,192 bytes copied during GC (scavenged)
465,051,008 bytes copied during GC (not scavenged)
382,705,664 bytes maximum residency (10 sample(s))

       3222 collections in generation 0 (  0.91s)
         10 collections in generation 1 ( 18.69s)

        742 Mb total memory in use

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    0.63s  (  0.71s elapsed)
  GC    time   19.60s  ( 20.73s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time   20.23s  ( 21.44s elapsed)

  %GC time      96.9%  (96.7% elapsed)

  Alloc rate    2,681,318,018 bytes per MUT second

  Productivity   3.1% of total user, 2.9% of total elapsed
</screen>

<para id="x_zJ1">
When using <code>-sstderr</code>, our program's performance numbers are
printed to the standard error stream, giving us a lot of information about
what our program was doing. In particular, it tells us how much time was
spent in garbage collection, and what the maximum live memory in use was. It
turns out that to compute the mean of a list of 10 million elements our
program used a maximum of 742 megabytes to the heap, and spent 96.9% of its time
doing garbage collection! In total, only 3.1% of the program's runtime was
spent doing productive work.
</para>

<para id="x_AK1">
So why is our program behaving so badly, and what can we do to improve it?
After all, Haskell is a lazy language: shouldn't it be able to process the
list in constant space?
</para>

</sect2>

<sect2>
  <title>Time profiling</title>

<para id="x_BK1">
GHC, thankfully, comes with several tools to analysing a program's time and
space usage. In particular, we can compile a program with profiling enabled,
which when run, yields useful information about what resources each function
was using. Profiling proceeds in three steps: compiling the program for
profiling; running it with particular profiling modes enabled; and inspecting
the resulting statistics.
</para>

<para id="x_CK1">
To compile our program for basic time and allocation profiling, we use the
<code>-prof</code> flag.  We also need to tell the profiling code which
functions we're interested in profiling, by adding "cost centres" to them.  A
cost center is a location in the program we'd like to collect statistics about,
and GHC will generate code to compute the cost of evaluation the expression
at each location. Cost centres can be added manually to any expression, using
the <code>SCC</code> pragma: </para>

&SCC.hs:scc;

<para id="x_DK1">
Alternatively, we can have the compiler insert the cost centres on all top
level functions for us by compiling with the <code>-auto-all</code> flag.
Manual cost centres are a useful addition to automated cost centre profiling,
as once a hot spot has been identified, we can precisely pin down the
expensive sub-expressions of a function.
</para>

<para id="x_EK1">
One complication to be aware of: in a lazy, pure language like Haskell, values
with no arguments need only be computed once (for example, the large list in
our example program), and the result shared for later uses. Such values are not
really part of the call graph of a program, as they're not evaluated on each
call, but we would of course still like to know how expensive their one-off
cost of evaluation was. To get accurate numbers for these values, known as
"constant applicative forms", or CAFs, we use the <code>-caf-all</code> flag.
</para>

<para id="x_FK1">
Compiling our example program for profiling then (using the
<code>-no-recomp</code> flag to to force full recompilation):
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 --make A.hs -prof -auto-all -caf-all -no-recomp</userinput>
[1 of 1] Compiling Main             ( A.hs, A.o )
Linking A ...
</screen>

<para id="x_GK1">
We can now run this annotated program with time profiling enabled (and we'll
use a smaller input size for the time being, as the program now has
additional profiling overheads):
</para>

<screen>
<prompt>$ </prompt><userinput>time ./A  1e6 +RTS -p</userinput>
Stack space overflow: current size 8388608 bytes.
Use `+RTS -Ksize' to increase it.
./A 1e6 +RTS -p  1.11s user 0.15s system 95% cpu 1.319 total
</screen>

<para id="x_HK1">
The program ran out of stack space! This is the main complication to be aware
of when using profiling: adding cost centres to a program modifies how it is
optimised, possibly changing its runtime behaviour, as each expression now
has additional code associated with it to track the evaluation steps. In a
sense, observing the program executing modifies how it executes. In this
case, it is simple to proceed, we use the GHC runtime flag, <code>-K</code>,
to set an arbitrarily large stack limit for our program: </para>

<screen>
<prompt>$ </prompt><userinput>time ./A 1e6 +RTS -p -K100M</userinput>
500000.5
./A 1e6 +RTS -p -K100M  4.27s user 0.20s system 99% cpu 4.489 total
</screen>

<para id="x_IK1">
The runtime will dump its profiling information into a file,
<code>A.prof</code>, named after the binary that was executed, which
contains the following information:
</para>

<screen>
Time and Allocation Profiling Report  (Final)

	   A +RTS -p -K100M -RTS 1e6

	total time  =        0.28 secs   (14 ticks @ 20 ms)
	total alloc = 224,041,656 bytes  (excludes profiling overheads)

COST CENTRE  MODULE               %time %alloc

CAF:sum      Main                  78.6   25.0
CAF          GHC.Float             21.4   75.0

                                            individual    inherited
COST CENTRE MODULE         no.    entries  %time %alloc   %time %alloc

MAIN        MAIN            1           0   0.0    0.0   100.0  100.0
 main       Main          166           2   0.0    0.0     0.0    0.0
  mean      Main          168           1   0.0    0.0     0.0    0.0
 CAF:sum    Main          160           1  78.6   25.0    78.6   25.0
 CAF:lvl    Main          158           1   0.0    0.0     0.0    0.0
  main      Main          167           0   0.0    0.0     0.0    0.0
 CAF        Numeric       136           1   0.0    0.0     0.0    0.0
 CAF        Text.Read.Lex 135           9   0.0    0.0     0.0    0.0
 CAF        GHC.Read      130           1   0.0    0.0     0.0    0.0
 CAF        GHC.Float     129           1  21.4   75.0    21.4   75.0
 CAF        GHC.Handle    110           4   0.0    0.0     0.0    0.0
</screen>

<para id="x_JK1">
This gives us a view into the program's runtime behaviour. We can see the
program's name and the flags we ran it with. The "total time" is time actually
spent executing code from the runtime system's point of view, and the total
allocation is the number of bytes allocated during the entire program run (not
the maximum live memory, which was around 700M).
</para>

<para id="x_KK1">
The second section of the profiling report is the proportion of time and space
each function was responsible for. The third section is the cost centre report,
structured as a call graph (for example, we can see that
<function>mean</function> was called from <function>main</function>. The
"individual" and "inherited" columns give us the resources a cost centre was
responsible for on its own, and what it and its children were responsible for.
Additionally, we see the one-off costs of evaluating constants, (such as the
floating point values in the large list, and the list itself), assigned to top
level CAFs.
</para>

<para id="x_LK1">
What conclusions can we draw from this information? We can see that the
majority of time is spent in two CAFs, one related to computing the sum, and
another for floating point numbers. These alone accounts for nearly all
allocations that occured during the program run. Combined with our earlier
observation about garbage collector stress, it begins to look like the list
node allocations, containing floating point values, are causing a problem.
</para>

<para id="x_MK1">
For simple performance hot spot identification, particularly in large programs
where we might have little idea where time is being spent, the initial time
profile can narrow down to a particular problematic module and top level
function, which is often enough to reveal the trouble spot. Once we've narrowed
down the code to a problematic section, such as our example here, we can use
more sophisticated profiling tools to extract more information.
</para>

</sect2>

<sect2>
  <title>Space profiling</title>

<para id="x_NK1">
Beyond basic time and allocation statistics, GHC is able to generate graphs
of memory usage of the heap, over the program's lifetime. This is perfect for
revealing "space leaks", where memory is retained unneccessarily, leading to
the kind of heavy garbage collector activity we see in our example.
</para>

<para id="x_OK1">
Constructing a heap profile follows the same steps as constructing a normal
time profile, namely, compile with <code>-prof -auto-all -caf-all</code>, but
when we execute the program, we'll ask the runtime system to gather more
detailed heap use statistics. We can break down the heap use information in
several ways: via cost-centre, via module, by constructor, by data type, each
with their own insights. Heap profiling produces a log to a file,
<code>A.hp</code>, with raw data which is in turn processed by the tool
<code>hp2ps</code>, which generates a PostScript-based, graphical
visualisation of the heap over time.
</para>

<para id="x_PK1">
To extract a standard heap profile from our program, we compile run it with
the <code>-hc</code> runtime flag:
</para>

<screen>
<prompt>$ </prompt><userinput>time ./A 1e6 +RTS -hc -p -K100M</userinput>
500000.5
./A 1e6 +RTS -hc -p -K100M  4.15s user 0.27s system 99% cpu 4.432 total
</screen>

<para id="x_QK1">
A heap profiling log, <code>A.hp</code>, was created, with the content in the
following form:
</para>

<screen>
JOB "A 1e6 +RTS -hc -p -K100M"
SAMPLE_UNIT "seconds"
VALUE_UNIT "bytes"
BEGIN_SAMPLE 0.00
END_SAMPLE 0.00
BEGIN_SAMPLE 0.24
(167)main/CAF:lvl   48
(136)Numeric.CAF    112
(166)main   8384
(110)GHC.Handle.CAF 8480
(160)CAF:sum    10562000
(129)GHC.Float.CAF  10562080
END_SAMPLE 0.24
</screen>

<para id="x_RK1">
Samples are taken at regular intervals during the program run. We can
increase the frequency of samples by using <code>-iN</code>, where N is the
fraction of a second to sample at. Obviously, the more we sample, the more
accurate the results, but the slower our program will run. We can now render
the heap profile as a graph, using the <code>hp2ps</code> tool:
</para>

<screen>
<prompt>$ </prompt><userinput>hp2ps -e8in -c A.hp</userinput>
</screen>

<para id="x_SK1">
This produces the graph, in the file <code>A.ps</code>:
</para>

<!-- -hc raw heap info -->

<informalfigure>
<graphic fileref="figs/ch26-heap-hc.png"/>
</informalfigure>

<para id="x_TK1">
What does this graph tell us? For one, the program runs in two phases: spending
its first half allocating increasingly large amounts of memory, while summing
values, and the second half cleaning up those values. The initial allocation
also coincides with <code>sum</code>, doing some work, allocating a lot of
data. We get a slightly different presentation if we break down the allocation
by type, using <code>-hy</code> profiling:
</para>

<screen>
<prompt>$ </prompt><userinput>time ./A 1e6 +RTS -hy -p -K100M</userinput>
500000.5
./A 1e6 +RTS -i0.001 -hy -p -K100M  34.96s user 0.22s system 99% cpu 35.237 total
$ hp2ps -e8in -c A.hp
</screen>

<para id="x_UK1">
Which yields the following graph:
</para>

<informalfigure>
<graphic fileref="figs/ch26-heap-hy.png"/>
</informalfigure>

<para id="x_VK1">
The most interesting things to notice here are large parts of the heap devoted
to values of list type (the <code>[]</code> band), and heap-allocated
<code>Double</code> values. There's also some heap allocated data of unknown
type. Finally, let's break it down by what constructors are being allocated,
using the <code>-hd</code> flag:
</para>

<screen>
<prompt>$ </prompt><userinput>time ./A 1e6 +RTS -hd -p -K100M</userinput>
$ time ./A 1e6 +RTS -i0.001 -hd -p -K100M 
500000.5
./A 1e6 +RTS -i0.001 -hd -p -K100M  27.85s user 0.31s system 99% cpu 28.222 total
</screen>

<para id="x_WK1">
Our final graphic reveals the full story of what is going on:
</para>

<informalfigure>
<graphic fileref="figs/ch26-heap-hd.png"/>
</informalfigure>

<para id="x_XK1">
A lot of work is going into allocating list nodes containing double precision
floating point values. Haskell lists are lazy, so the full million element
list is built up over time. Crucially, though, it is not being deallocated as it is
traversed, leading to increasingly large resident memory use. Finally, a bit
over half our way through the program run, the program finally finishes
summing the list, and starts calculating the length. If we look at the
original fragment for <code>mean</code>, we can see exactly why that memory
is being retained.
</para>

&Fragment.hs:fragment;

<para id="x_YK1">
At first we sum our list, which triggers the allocation of list nodes, but
we're unable to release the list nodes once we're done, as the entire list is
still needed by <function>length</function>. As soon as
<function>sum</function> is done though, and <function>length</function>
starts consuming the list, the garbage collector can chase it along,
deallocating the list nodes, until we're done. These two phases of evaluation
give two strikingly different phases of allocation and deallocation, and
point at exactly what we need to do: traverse the list only once, summing and
averaging it as we go.
</para>

</sect2>

</sect1>

<sect1>
  <title>Controlling evaluation</title>

<para id="x_ZK1">
We have a number of options if we want to write our loop to traverse the list
only once. For example, we can write the loop as a fold over the list, or via
explicit recursion on the list structure. Sticking to the high level
approaches, we'll try a fold first:
</para>

&B.hs:fold;

<para id="x_aK1">
Now, instead of taking the sum of the list, and retaining the list till we can
take its length,  we left-fold over the list accumulating the intermediate sum
and length values in a pair (and we must left fold, since a right fold would
take us to the end of the list and work backwards, which is exactly not what
we need in this case).
</para>

<para id="x_bK1">
The body of our loop is the <function>k</function> function, which takes the
intermediate loop state, and the current element, and returns a new state with
the length increased by one, and the sum increased by the current element. When
we run this, however, we get surprising results: </para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 --make B.hs -no-recomp</userinput>
<prompt>$ </prompt><userinput>time ./B 1e6</userinput>
Stack space overflow: current size 8388608 bytes.
Use `+RTS -Ksize' to increase it.
./B 1e6  0.44s user 0.10s system 96% cpu 0.565 total
</screen>

<para id="x_cK1">
We traded wasted heap for wasted stack! In fact, if we increase the stack size
to the size of the heap in our previous implementation, with the
<code>-K</code> runtime flag, the program runs to completion, and has similar
allocation figures:
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 --make B.hs -prof -auto-all -caf-all -no-recomp</userinput>
[1 of 1] Compiling Main             ( B.hs, B.o )
Linking B ...
<prompt>$ </prompt><userinput>time ./B 1e6 +RTS -i0.001 -hc -p -K100M</userinput>
500000.5
./B 1e6 +RTS -i0.001 -hc -p -K100M  38.70s user 0.27s system 99% cpu 39.241 total
</screen>

<para id="x_dK1">
Generating the heap profile, we see all the allocation is now in <code>mean</code>:
</para>

<informalfigure>
<graphic fileref="figs/ch26-stack.png"/>
</informalfigure>

<para id="x_eK1">
The question is: why are we building up more and more allocated state, when all
we are doing is folding over the list? This, it turns out, is a classic space
leak due to excessive laziness.
</para>

<sect2>
    <title>Strictness and tail recursion</title>

<para id="x_fK1">
The problem is that our left fold, <function>foldl</function> is too lazy.
What we want is a tail recursive loop, which can be implemented effectively
as a <code>goto</code>, where no state is left on the stack. In this case
though, rather than fully reducing the tuple state at each step, a long chain
of thunks is being created, that only towards the end of the program is
evaluated.  At no point do we demand reduction of the loop state, so the
compiler is unable to infer any strictness, and must reduce the value purely
lazily.
</para>

<para id="x_gK1">
What we need to do is to tune the evaluation strategy slightly: lazily
unfolding the list, but strictly accumulating the fold state. The standard
approach here is to replace <function>foldl</function> with
<function>foldl'</function>, from the <code>Data.List</code> module:
</para>

&C.hs:fold;

<para id="x_hK1">
However, if we run this implementation, we see we still haven't quite got it
right:
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 --make C.hs</userinput>
[1 of 1] Compiling Main             ( C.hs, C.o )
Linking C ...
<prompt>$ </prompt><userinput>time ./C 1e6</userinput>
Stack space overflow: current size 8388608 bytes.
Use `+RTS -Ksize' to increase it.
./C 1e6  0.44s user 0.13s system 94% cpu 0.601 total
</screen>

<para id="x_iK1">
Still not strict enough! Our loop is continuing to accumulate unevaluated
state on the stack. The problem here is that <function>foldl'</function> is
only outermost strict:
</para>

&Foldl.hs:fold;

<para id="x_jK1">
This loop uses <function>`seq`</function> to reduce the accumulated state at
each step, but only to the outermost constructor on the loop state. That is,
<code>seq</code> reduces an expression to "weak head normal form". Evaluation
stops on the loop state once the first constructor is reached. In this case,
the outermost constructor is the tuple wrapper, <code>(,)</code>, which
isn't deep enough. The problem is still the unevaluated numeric state inside
the tuple.
</para>
</sect2>

<sect2>
  <title>Adding strictness</title>

<para id="x_kK1">
There are a number of ways to make this function fully strict. We can, for
example, add our own strictness hints to the internal state of the tuple,
yielding a truly tail recursive loop:
</para>

&D.hs:fold;

<para id="x_lK1">
In this variant, we step inside the tuple state, and explicitly tell the
compiler that each state component should be reduced, on each step. This
gives us a version that does, at last, run in constant space:
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 D.hs --make</userinput>
[1 of 1] Compiling Main             ( D.hs, D.o )
Linking D ...
</screen>

<para id="x_mK1">
If we run this, with allocation statistics enabled, we get the satisifying
result:
</para>

<screen>
<prompt>$ </prompt><userinput>time ./D 1e6 +RTS -sstderr</userinput>
./D 1e6 +RTS -sstderr 
500000.5
256,060,848 bytes allocated in the heap
     43,928 bytes copied during GC (scavenged)
     23,456 bytes copied during GC (not scavenged)
     45,056 bytes maximum residency (1 sample(s))

        489 collections in generation 0 (  0.00s)
          1 collections in generation 1 (  0.00s)

          1 Mb total memory in use

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    0.12s  (  0.13s elapsed)
  GC    time    0.00s  (  0.00s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time    0.13s  (  0.13s elapsed)

  %GC time       2.6%  (2.6% elapsed)

  Alloc rate    2,076,309,329 bytes per MUT second

  Productivity  97.4% of total user, 94.8% of total elapsed

./D 1e6 +RTS -sstderr  0.13s user 0.00s system 95% cpu 0.133 total
</screen>

<para id="x_nK1">
Unlike our first version, this program is 97.4% efficient, spending only 2.6%
of its time doing garbage collection, and it runs in a constant 1 megabyte of
space. It illustrates a nice balance between mixed strict and lazy
evaluation, with the large list unfolded lazily, while we walk over it,
strictly. The result is a program that runs in constant space, and does so quickly.
</para>

<sect3>
  <title>Normal form reduction</title>

<para id="x_oK1">
There are a number of other ways we could have addressed the strictness issue
here. For deep strictness, we can use the <code>rnf</code> function, part of
the parallel strategies library, which unlike <code>seq</code> reduces to
normal form (hence its name). Such a "deep seq" fold we can write as:
</para>

&E.hs:fold;

<para id="x_pK1">
We change the implementation of <code>foldl'</code> to reduce the state to
normal form, using the <code>rnf</code> strategy. This also raises an issue
we avoided earlier: the type inferred for the loop accumulator state.
Previously, we relied on type defaulting to infer a numeric, integral type
for the length of the list in the accumulator, but switching to
<code>rnf</code> introduces the <code>NFData</code> class constraint, and we
can no longer rely on defaulting to set the length type.
</para>

</sect3>

<sect3>
  <title>Bang patterns</title>

<para id="x_qK1">
Perhaps the cheapest way, syntactically to add required strictness to code
that's excessively lazy is via "bang patterns", a language extension
introduced with the following pragma:
</para>

&F.hs:pragma;

<para id="x_rK1">
With bang patterns, we can hint at strictness on any binding form, making the
function strict in that variable. Much as explicit type annotations can
guide type inference, bang patterns can help guide strictness inference.
We can now rewrite the loop state to be simply:
</para>

&F.hs:fold;

<para id="x_sK1">
The intermediate values in the loop state are now made strict, and the loop
runs in constant space:
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 F.hs --make</userinput>
<prompt>$ </prompt><userinput>time ./F 1e6 +RTS -sstderr</userinput>
./F 1e6 +RTS -sstderr 
500000.5
256,060,848 bytes allocated in the heap
     43,928 bytes copied during GC (scavenged)
     23,456 bytes copied during GC (not scavenged)
     45,056 bytes maximum residency (1 sample(s))

        489 collections in generation 0 (  0.00s)
          1 collections in generation 1 (  0.00s)

          1 Mb total memory in use

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    0.14s  (  0.15s elapsed)
  GC    time    0.00s  (  0.00s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time    0.14s  (  0.15s elapsed)

  %GC time       0.0%  (2.3% elapsed)

  Alloc rate    1,786,599,833 bytes per MUT second

  Productivity 100.0% of total user, 94.6% of total elapsed

./F 1e6 +RTS -sstderr  0.14s user 0.01s system 96% cpu 0.155 total
</screen>

<para id="x_tK1">
In large projects, when we investigating memory allocation hot spots, bang
patterns are the cheapest way to speculatively modify the strictness
properties of some code, as they're syntactically less invasive than other
methods.
</para>

</sect3>

<sect3>
<title>Strict data types</title>

<para id="x_uK1">
Strict data types are another effective way to provide strictness information
to the compiler. By default, Haskell data types are fully lazy, but it is
easy enough to to add strictness information to the fields of a data type
that then propagate through the program. We can declare a new strict pair
type, for example:
</para>

&G.hs:pair;

<para id="x_vK1">
This creates a pair type whose fields will always be kept in weak head normal
form. We can now rewrite our loop as:
</para>

&G.hs:fold;

<para id="x_wK1">
This implementation again has the same efficient, constant space behaviour.
At this point, to squeeze the last drops of performance out of this code,
though, we have to dive a bit deeper.
</para>

</sect3>

</sect2>

</sect1>

<sect1>
    <title>Understanding Core</title>

<para id="x_xK1">
Besides looking at runtime profiling data, one sure way of determining
exactly what your program is doing is to look at the final program source after
the compiler is done optimising it, particularly in the case of Haskell
compilers, which can perform very aggressive transformations on the code. GHC
uses "a simple functional language", known as Core, as the compiler
intermediate representation. It is essentially a subset of Haskell, augmented
with unboxed data types (these are raw machine types, directly corresponding
to primitive data types in languages like C), suitable for code generation.
GHC optimises Haskell by transformation, repeatedly rewriting the source into
more and more efficient forms. The Core representation is the final version
of your program, and the one that will be executed on the machine. In other
words, Core has the final say, and if all out performance is your goal, it is
worth understanding.
</para>

<para id="x_yK1">
To view the Core our Haskell program we compile with the
<code>-ddump-simpl</code> flag, or use the <code>ghc-core</code> core tool, a
third party utility that lets us view Core in a pager. So let's look at the
representation of our final <code>fold</code> using strict data types, in
Core form:
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -O2 -ddump-simpl G.hs</userinput>
</screen>

<para id="x_zK1">
A screenful of text is generated, which if we look carefully at, we'll see a
loop (here, cleaned up slightly for clarity):
</para>

<screen>
lgo :: Integer -> [Double] -> Double# -> (# Integer, Double #)

lgo = \ n xs s ->
    case xs of
      []       -> (# n, D# s #);
      (:) x ys ->
        case plusInteger n 1 of
            n' -> case x of
                D# y -> lgo n' ys (+## s y)
</screen>

<para id="x_AL1">
This is the final version of our <code>foldl'</code>, and tells us a lot
about the next steps for optimisation. The fold itself has been entirely
inlined, yielding an explicit recursive loop over the list. The loop state,
our strict pair, has disappeared entirely, and the function now takes its 
length and sum accumulators as direct arguments along with the list.
</para>

<para id="x_BL1">
The sum of the list elements is represented with an unboxed
<code>Double#</code> value, a raw machine <code>double</code> kept in a
floating point register. This is ideal, as there will be no memory traffic
involved keeping the sum on the heap. However, the length of the list, since
we gave no explicit type annotation, has been inferred to be a heap-allocated
<code>Integer</code>, with requires a non-primitive <code>plusInteger</code>
to perform addition. If it is algorithmically sound to use a <code>Int</code>
instead, we can replace <code>Integer</code> with it, and GHC will then be
able to use a raw machine <code>int</code> for the length. We can hope for an
improvement in time and space by ensuring both loop components are unboxed,
and kept in registers.
</para>

<para id="x_CL1">
The base case of the loop, its end, yields an unboxed pair (a pair allocated
only in registers), storing the final length of the list, and the accumulated
sum. Notice that the return type is a a heap-allocated <code>Double</code>
value, indicated by the <code>D#</code> constructor, which lifts a raw double
value onto the heap, and again this has implications for performance, as GHC
will need to check there is sufficient heap space available before it can
allocate and return from the loop.
</para>

<para id="x_DL1">
We can avoid this final heap check by having GHC return an unboxed
<code>Double#</code> value, which can be achieved by using a custom pair type
in the loop. In addition, can can enable an optimisation GHC provides, that
unboxes the strict fields of a data type, which will ensure the new pair data
type is stored entirely in registers. This optimisation is turned on with
<code>-funbox-strict-fields</code>.
</para>

<para id="x_EL1">
We can make both representation changes by replacing the polymorphic strict
pair type with one whose fields are fixed as <code>Int</code> and
<code>Double</code>:
</para>

&H.hs:fold;

<para id="x_FL1">
Compiling this with optimisations on, and <code>-funbox-strict-fields -ddump-simpl</code>,
we get a tighter inner loop in Core:
</para>

<screen>
lgo :: Int# -> Double# -> [Double] -> (# Int#, Double# #)
lgo = \ n s xs ->
    case xs of
      []       -> (# n, s #)
      (:) x ys ->
        case x of 
            D# y -> lgo (+# n 1) (+## s y) ys
</screen>

<para id="x_GL1">
Now the pair we use to represent the loop state is represented and returned as
unboxed primitive types, and will be kept in registers. The final version now
only allocates heap memory for the list nodes, as the list is lazily demanded.
If we compile and run this tuned version, we can compare the allocation and time 
performance against our original program, with the full input size:
</para>

<screen>
<prompt>$ </prompt><userinput>time ./H 1e7 +RTS -sstderr</userinput>
./H 1e7 +RTS -sstderr 
5000000.5
1,689,133,824 bytes allocated in the heap
    284,432 bytes copied during GC (scavenged)
         32 bytes copied during GC (not scavenged)
     45,056 bytes maximum residency (1 sample(s))

       3222 collections in generation 0 (  0.01s)
          1 collections in generation 1 (  0.00s)

          1 Mb total memory in use

  INIT  time    0.00s  (  0.00s elapsed)
  MUT   time    0.63s  (  0.63s elapsed)
  GC    time    0.01s  (  0.02s elapsed)
  EXIT  time    0.00s  (  0.00s elapsed)
  Total time    0.64s  (  0.64s elapsed)

  %GC time       1.0%  (2.4% elapsed)

  Alloc rate    2,667,227,478 bytes per MUT second

  Productivity  98.4% of total user, 98.2% of total elapsed

./H 1e7 +RTS -sstderr  0.64s user 0.00s system 99% cpu 0.644 total
</screen>

<para id="x_HL1">
While our original program, when operating on a list of 10 million elements,
took more than a minute to run, and allocated more than 700 megabytes of memory,
the final version, using a simple higher order fold, and a strict data type, 
runs in around half a second, and allocates a total of 1 megabyte. Something of
an improvement!
</para>

<para id="x_IL1">
The general rules we can learn from the profiling and optimisation process are:
</para>

  <itemizedlist>
    <listitem>
      <para id="x_JL1">Compile to native code, with optimisations on</para>
    </listitem>
    <listitem>
      <para id="x_KL1">When in doubt, use runtime statistics, and time profiling</para>
    </listitem>
    <listitem>
      <para id="x_LL1">If allocation problems are suspected, use heap profiling</para>
    </listitem>
    <listitem>
      <para id="x_ML1">A careful mixture of strict and lazy evaluation can yield the best results</para>
    </listitem>
    <listitem>
      <para id="x_NL1">Prefer strict fields for atomic data types (<code>Int</code>,
        <code>Double</code> and similar types)</para>
    </listitem>
    <listitem>
      <para id="x_OL1">Use data types closer to the machine representation (prefer
        <code>Int</code> over <code>Integer</code>)</para>
    </listitem>

  </itemizedlist>

<para id="x_PL1">
These simple strategies are enough to identify and squash untoward memory use
issues, and when used wisely, can avoid them occuring in the first place.
</para>

</sect1>

<sect1>
  <title>Advanced techniques: fusion</title>

<para id="x_QL1">
The final bottleneck in our program is the lazy list itself. While we can
avoid allocating it all at once, there is still memory traffic each time
around the loop, as we demand the next cons cell in the list, allocate it to
the heap, operate on it, and continue. The list type is also polymorphic, so
the elements of the list must be represented as heap allocated
<code>Double</code> values.
</para>

<para id="x_RL1">
What we'd like to do is eliminate the list entirely, keeping just the next
element we need in a register. Perhaps surprisingly, GHC is able to transform
the list program into a listless version, using an optimisation known as
deforestation, which refers to a general class of optimisations that involve
eliminating intermediate data structures. Due to the absence of side effects,
a Haskell compiler can be extremely aggressive when rearranging code,
reordering and transforming code wholesale at times. The specific
deforestation optimisation we can use here is stream fusion.
</para>

<para id="x_SL1">
This optimisation transforms recursive list generation and transformation
functions into non-recursive <code>unfolds</code>. When an
<code>unfold</code> appears next to a <code>fold</code>, the structure between
them is then eliminated entirely, yielding a single, tight loop, with no heap
allocation. The optimisation isn't enabled by default, but is enabled by a
number of data structure libraries, which provide "rewrite rules", custom
optimisations the compiler applies to functions the library exports. 
</para>

<para id="x_TL1">
We'll use the <code>uvector</code> library, which provides a suite of
list-like operations that use stream fusion to remove intermediate data
structures. Rewriting our program to use streams is straight forward:
</para>

&I.hs:fold;

<para id="x_UL1">
After installing the <code>uvector</code> library, from Hackage, we can build
our program, with <code>-O2 -funbox-strict-fields</code>, and inspect the Core
that results:
</para>

<screen>
fold :: Int# -> Double# -> Double# -> (# Int#, Double# #)
fold = \ n s t ->
    case >## t limit of {
      False -> fold (+# n 1) (+## s t) (+## t 1.0)
      True  -> (# n, s #)
</screen>

<para id="x_VL1">
This is really the optimal result! Our lists have been entirely fused away,
yielding a tight loop where list generation is interleaved with acumulation,
and all input and output variables are kept in registers. Running this,
we see another improvement bump in performance, with runtime falling by
another order of magnitude:
</para>

<screen>
$ time ./I 1e7
5000000.5
./I 1e7  0.06s user 0.00s system 72% cpu 0.083 total
</screen>

<sect2>
    <title>Tuning the generated assembly</title>

<para id="x_WL1">
Given that our Core is now optimal, the only step left to take, to optimise this
program further, is by looking directly at the assembly. Of course, there
are only small gains left to make at this point. To view the generated
assembly, we can use a tool like <code>ghc-core</code>, or generate assembly
to standard output with the <code>-ddump-asm</code> flag to GHC. We have few
levers available to adjust the generated assembly, but we may choose between
the C and native code backends to GHC, and, if we choose the C backend, which
optimisation flags to pass to GCC. Particularly with floating point code, it
is sometimes useful to compile via C, and enable specific high performance C
compiler optimisations.  
</para>

<para id="x_XL1">
For example, we can squeeze our the last drops of performance
from our final fused loop code by using <code>-funbox-strict-fields -fvia-C -optc-O2</code>, which
cuts the running time in half again (as the C compiler is able to optimise
away some redundant move instructions in the program's inner loop):
</para>

<screen>
<prompt>$ </prompt><userinput>ghc -no-recomp --make -O2 -funbox-strict-fields -fvia-C -optc-O2 I.hs</userinput>
[1 of 1] Compiling Main             ( I.hs, I.o )
Linking I ...
<prompt>$ </prompt><userinput>time ./I 1e7</userinput>
5000000.5
./I 1e7  0.04s user 0.00s system 98% cpu 0.047 total
</screen>

<para id="x_YL1">
Inspecting the final assembly (via <code>-keep-tmp-files</code>), we
see the generated loop contains only six instructions:
</para>

<screen>
go:
  ucomisd     5(%rbx), %xmm6
  ja  .L31
  addsd       %xmm6, %xmm5
  addq        $1, %rsi
  addsd       .LC0(%rip), %xmm6
  jmp go
</screen>

<para id="x_ZL1">
We've effectively massaged the program through multiple source level
optimisations, all the way to the final assembly. There's no where else to go from
here. Optimising code to this level is very rarely necessarily, of course,
and typically makes sense when writing low level libraries, or optimising
particularly important code, where all algorithm choices have already
determined. For day to day code, choosing better algorithms is always a more
effective strategy, but it's useful to know we can optimise down to the metal
if necessary.
</para>

</sect2>

<sect2>
  <title>Conclusions</title>

<para id="x_aL1">
In this chapter we've looked at a suite of tools and techniques you can use to
track down and identify problematic areas of your code, along with a variety of
conventions that can go a long way towards keeping code lean and efficient.
The goal is really to program in such a way that you have good knowledge of
what your code is doing, at all levels from source, through the compiler, to
the metal, and be able to focus in on particular levels when requirements demand.
</para>

<para id="x_bL1">
By sticking to simple rules, choosing the right data structures, and avoiding
the traps of the unwary, it is perfectly possible to reliably achieve high
performance from your Haskell code, while being able to develop at a very
high level. A balance of productivity and ruthless efficiency.
</para>

</sect2>

</sect1>

</chapter>

<!--
local variables: 
sgml-parent-document: ("00book.xml" "book" "chapter")
end:
-->
